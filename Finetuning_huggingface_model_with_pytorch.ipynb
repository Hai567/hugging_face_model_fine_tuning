{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMrJCZ/fIUbyLunwb0yJF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hai567/hugging_face_model_fine_tuning/blob/main/Finetuning_huggingface_model_with_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW-Xu6023R2d"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "!wget \"https://storage.googleapis.com/kagglesdsdata/datasets/2464675/4177116/Poem_classification%20-%20train_data.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240610T083717Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=26255cc796015419862cbd4a968740821a661b951e0df18647a3ba622141896ec65e1a2aa772fe178917290b2f08741356c80abd0568cc3185e6623fad4ab06162176d31f7516ae742ddce3217b671fbae5a07ded1a5a583148f0fe9cd4198d86667cac22d06c8b3615a0d500c89524113d3b5d6ca4b756dcb5097ac996ff3840a839a5a6d3a8f63b9b76716790656d14144e83dbc0f455fd2422957f1c1db97a0a338ac13e9be27f9a6beac97ea5774721f6eb1c5070989cb0411f3857f36f3c19047cf2869c6e3d7194c09686c9fafa4f9d722e02d5998dd58942bfd8b958c548e8c2ca46d5e002b71490870cac5f8546500e1188f4ec996851f64519acaf8\" -o \"train_data.csv\"\n",
        "!wget \"https://storage.googleapis.com/kagglesdsdata/datasets/2464675/4177116/Poem_classification%20-%20test_data.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240610T083756Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=5f9696836d0384b66c5c1068c0cd58252211c349810c4b4de1fc3a4643e2c210a3cd7da8025d893a7851ae098ebe4ee692bb324c86653561ce4762dc15d68b207fe618780027f6caacd6fbcef1cb1cbda5c34912316fda247b26b0623249af226a6a840641e8212c888ac785b08d859755989d280a0f732d252900b0e770320a25e41e7f05ea5221a0f60caebb0c062ceab447b37e623f6dc4c40d45d1c6f75c039b007dbd944ad1125d31a677a1bd39762989c47f800bdc0c59322b799488b11b62b5a68bad263ee1e126962a79fcf28e15b50bc340a7ef06090b72a8816f0468ef83949a0ab590947e742e3d5475e06b34ad5d377468ab763b85b9884d6b84\" -o \"test_data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "name = \"distilbert/distilbert-base-uncased\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)"
      ],
      "metadata": {
        "id": "DQ3O_uM9P_Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b2557b-a1b2-48bb-db74-b4065fc28271"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(\"./test_data.csv\")\n",
        "train_data = pd.read_csv(\"./train_data.csv\")\n",
        "\n",
        "test_data.dropna(inplace=True)\n",
        "train_data.dropna(inplace=True)\n",
        "\n",
        "X_train = train_data.drop(labels=\"Genre\", axis=1)\n",
        "Y_train = train_data[\"Genre\"]\n",
        "\n",
        "X_test = test_data.drop(labels=\"Genre\", axis=1)\n",
        "Y_test = test_data[\"Genre\"]\n",
        "\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size=0.3)\n",
        "\n",
        "\n",
        "\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "X_val = X_val.values\n",
        "# Y_train = Y_train.values\n",
        "# Y_test = Y_test.values\n",
        "# Y_val = Y_val.values\n",
        "\n",
        "# Flatten the list using numpy\n",
        "X_train = np.concatenate(X_train).tolist()\n",
        "X_test = np.concatenate(X_test).tolist()\n",
        "X_val = np.concatenate(X_val).tolist()\n",
        "# # Flatten the list using itertools\n",
        "# X_train = list(itertools.chain.from_iterable(X_train))\n",
        "# # Flatten the list using sum\n",
        "# X_train = sum(X_train, [])\n",
        "\n",
        "\n",
        "Y_train = Y_train.replace(\"Music\", 0).replace(\"Death\", 1).replace(\"Affection\", 2).replace(\"Environment\", 3).tolist()\n",
        "Y_test = Y_test.replace(\"Music\", 0).replace(\"Death\", 1).replace(\"Affection\", 2).replace(\"Environment\", 3).tolist()\n",
        "Y_val = Y_val.replace(\"Music\", 0).replace(\"Death\", 1).replace(\"Affection\", 2).replace(\"Environment\", 3).tolist()"
      ],
      "metadata": {
        "id": "UK6rSbSQQweB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, text_poems, array_labels, tokenizer):\n",
        "        self.poems = text_poems\n",
        "        self.labels = array_labels\n",
        "        self.encoder = tokenizer\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            encoded_poem = self.encoder(self.poems[idx], padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "            input_ids = encoded_poem[\"input_ids\"]\n",
        "            attention_mask = encoded_poem[\"attention_mask\"]\n",
        "\n",
        "            item = {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"labels\": torch.tensor(self.labels[idx])  # Ensure labels are converted to tensors\n",
        "            }\n",
        "\n",
        "            return item\n",
        "        except Exception as e:\n",
        "            print(f\"Error at index {idx}: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "CMajwiDl2azQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "train_dataset = MyDataset(X_train, Y_train, tokenizer)\n",
        "test_dataset = MyDataset(X_test, Y_test, tokenizer)\n",
        "val_dataset = MyDataset(X_val, Y_val, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=200)\n",
        "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=200)\n",
        "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=200)"
      ],
      "metadata": {
        "id": "OKg75pq668dr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "losses = []\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for e in range(epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].squeeze().to(device)\n",
        "        attention_mask = batch['attention_mask'].squeeze().to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        print(outputs)\n",
        "        # loss = outputs.loss\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        # losses.append(loss)\n",
        "        # print(f\"Epoch {e}/{epochs} \\t loss: {loss}\")"
      ],
      "metadata": {
        "id": "VCoSKQ_67ScG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}